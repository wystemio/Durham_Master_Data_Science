# PFDS -- COMP42315 Programming for Data Science
*[Lecturer: Dr Hubert Shum](http://hubertshum.com)*  
*[Lecturer: Dr Gagangeet Aujla](https://www.dur.ac.uk/research/directory/staff/?mode=staff&id=19534)* 

## Content
- Lecture 01a Module Introduction
- Lecture 01b Web Programming Basics for Web Scraping
- Lecture 02 Website Analysis and Python Preliminaries
- Lecture 03 Web Scraping
- Lecture 04 Web Crawling and Data Munging
- Lecture 05 Advanced Web Programming Frameworks and How They Affect Scraping
- Lecture 06 Designing and Implementing Web Scraping Solutions

- file/data management 
- Data preparation 
- Correlation in Data Science using Python
- Data Analysis
- Complex Data Analysis
- Data Visualisation
 
 ## FEEDBACK for *ICS username*
 ### Total mark: 84.7/100
 *IMPORTANT* If you have submitted late, be aware that the mark above has not yet included the late submission caps. Mark capping will be done as a separate process by the school office.
 
 
 ### [Question Q1 - Citation Scrapping from Crawled Papers]
 
 #### Mark: 17.5/25
 
 Comment: Overall, the student has presented an excellent solution for Q1. First of all, the solution proposed crawls all the unique paper titles, thereby fulfilling the first main objective of the task. The solution correctly crawls the respective citations of all papers, which requires going into the publication detail pages of all all papers. The second main objective of the task is fulfilled. The solution sorts and visualises the top 25 cited papers according to the decending citation number, and therefore fulfils the third main objective of the question. The beginning part of the URL (i.e. the domain and folder) is stored in multiple hardcoded variables - this is less optimal as it would make future code management more difficult. A better way is to use a single variable to store them, and to use string operations to obtain the needed part of the URL when needed. This way, users only need to manage one input string in case the website address is updated, such as changing from https to http. Regarding the source code, the student has made some good uses of comments in this question, making the code easier to follow. 
 
 General comment to the class: Overall, the class has performed well on this question. The average score is 16.6/25. Many students are able to identify the unique paper titles and obtain the corresponding citation numbers, thereby sorting and displaying the results. Students generally comment on the source code well. More can be done in improving the future-proof aspect of the soft code.
 
 ### [Question Q2 - LDO Scrapping from Crawled Papers]
 
 #### Mark: 21/25
 
 Comment: You produced a legible scatter plot of the average number of LDO items per year. You showed the name and number of LDO items for the 'top' 25 publications in a legible table. You explained the design of your crawler and highlighted the features to navigate the website. You did not go above and beyond the text of the question to demonstrate mastery of the course content.
 
 General comment to the class: Overall, the class approached this question with rigour. Primary technical mistakes are not including the embedded YouTube videos as LDO items, or not checking the other Topic headings for additional content. Likewise, many have not presented what was asked in the report â€” please read directions carefully. Very few have even attempted to go above and beyond the text of the question to demonstrate mastery of the course content, the few who have made some effort in that regard should be pleased.
 
### [Question Q3 - Webpage Data Analysis and Visualisation]
 
#### Mark: 28/30
 
 Comment: The solution was built, deployed, and worked in a perfect manner. The use of visual is excellent, but the use of visual choices are limited. The report is well-written and structured, and the provided explanation is perfect, however it is written in a complicated manner. Moreover, it does not fully describe what was implemented.
 
 General comment to the class: The class has performed quite well in this question. The average score for this question is 20.7/30, which means that most of the students can answer the question fully. Students generally can anlayse and visualise allk the tasks, It is also good to see that different strategies have been proposed to analyse the problem suggested.
 
### [Question Q4 - Covid Data Dataset Analysis and Visualisation]
 
#### Mark: 18.2/20
 
 Comment: DATASET IMPORTING and LOADING:100/100
 All required dependencies have been imported and the dataset was correctly loaded and required functions implemented
 MODEL BUILDING:100/100
 The model was perfectly built, search criteria, DAG and fitting work perfectly
 MODEL PLOTTING AND SCORING: 100/100
 The model is perfectly plotted and scored
 REPORT: 55/100
 Report is well written and structured, the provided explanation is sufficient, however it does not fully described what was implemented 
 
 General comment to the class: All in all, very good marks for question 4. Most marks fall in the pass to excellent categories.. Some outliers, those close to the zero mark were submissions that did not contain software, or in some cases not even a report section for the question.
 I have to say I was very disappointed with the reports. Quite a few were badly formatted, some missing even a heading page with information about the student. Most have included too much graphics, irrelevant in most cases, given the software clearly has the require infographics. Formatting, for most reports was all over the place, showing the student does not know yet how to mix text and graphics. Some reports had a very odd choice of font type and a mixture of font sizes. The submitted code was reasonably good across the cohort, however more description should have been included.



